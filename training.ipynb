{"cells":[{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"05a9688117bb4b9b9a2e8393d27881b9","deepnote_cell_type":"text-cell-h1"},"source":"# ML4IoT - HW2","block_group":"46fc381636b947e9bbb611ab095a26c5"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1a18661b6daf418ca25d6ea5d2235ccf","deepnote_cell_type":"text-cell-p"},"source":"Team 1: Homayoun Afshari (s308563), Marcelo Bastos Ferreira (s308964), Gustavo Nicoletti Rosa (s317672)","block_group":"b13ab1c9efa949bd8daf9a58613feefa"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"79c8ac471a6d4700ad9d57c749110287","deepnote_cell_type":"text-cell-h1"},"source":"# Notebook Initialization","block_group":"329b7a22b4364248813d6c37d29e6f98"},{"cell_type":"code","metadata":{"source_hash":"57d89531","execution_start":1704123934476,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"1439d03572504d51beec72e5e77b67ef","deepnote_cell_type":"code"},"source":"### initial variables\nROOT_FOLDER = '.'\nDATABASE_FOLDER = f'/tmp'\nMODEL_NAME = 'model1'","block_group":"a97cfc0c90714148b0232fb020fa29ca","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"88df8cf4","execution_start":1704123934477,"execution_millis":2825,"deepnote_to_be_reexecuted":false,"cell_id":"f2a647950dcd4f14ae703b22acc37b6c","deepnote_cell_type":"code"},"source":"import sys\nimport os\nimport shutil\nimport zipfile\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport tensorflow_model_optimization as tfmot\nimport wandb\nfrom time import time\nfrom preprocessing import LABELS\nfrom preprocessing import AudioReader\nfrom preprocessing import MelSpectrogram\nfrom preprocessing import MFCC","block_group":"f2a647950dcd4f14ae703b22acc37b6c","execution_count":2,"outputs":[{"name":"stderr","text":"2024-01-01 15:45:34.852234: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-01 15:45:34.883710: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-01 15:45:34.884561: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-01-01 15:45:35.752251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"34ac9bf311594351aab068f2505e6888","deepnote_cell_type":"text-cell-h1"},"source":"# The Reference Model","block_group":"70b4bb20076a40ab8e07a3f21841b117"},{"cell_type":"code","metadata":{"source_hash":"818ba5f9","execution_start":1704123937309,"execution_millis":96,"deepnote_to_be_reexecuted":false,"cell_id":"d5a75fe97b68489f9a78ab2a70475e1e","deepnote_cell_type":"code"},"source":"REF_PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n}\n\ntflite_models_dir = f'{ROOT_FOLDER}/tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)\ntflite_model_name = os.path.join(tflite_models_dir, 'ref_model.tflite')\n\nif not os.path.exists(tflite_model_name):\n    ref_model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=[49, 40, 1]),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(units=2),\n        tf.keras.layers.Softmax()\n    ])\n\n    ref_model.build()\n\n    saved_model_dir = f'{ROOT_FOLDER}/saved_models/ref_model'\n    if not os.path.exists(saved_model_dir):\n        os.makedirs(saved_model_dir)\n    ref_model.save(saved_model_dir)\n\n    converter = tf.lite.TFLiteConverter.from_saved_model(f'{ROOT_FOLDER}/saved_models/ref_model')\n    tflite_model = converter.convert()\n\n    with open(tflite_model_name, 'wb') as fp:\n        fp.write(tflite_model)","block_group":"8384fd563ccd4b1cbc638baa0071fd88","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"34dbf23ba9a84385b6f70d9a012c1476","deepnote_cell_type":"text-cell-h1"},"source":"# Functions","block_group":"7d3c5457d9fc40259ac32faca2a8a2e4"},{"cell_type":"code","metadata":{"source_hash":"973dd325","execution_start":1704123937409,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"39ac621e19a940b8b3ff15a6cc861da7","deepnote_cell_type":"code"},"source":"def compare_to_reference(tflite_model_name, PREPROCESSING_ARGS):\n\n    ### generating a random audio\n    audio = tf.random.normal((16000,))\n\n    ### reference model's latency\n    mel_spec_processor = MelSpectrogram(**REF_PREPROCESSING_ARGS)\n    interpreter = tf.lite.Interpreter(model_path=f'{ROOT_FOLDER}/tflite_models/ref_model.tflite')\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    ref_latencies = []\n    for i in range(100):\n        start_preprocess = time()\n        log_mel_spectrogram = mel_spec_processor.get_mel_spec(audio)\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n        interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[0]['index'])\n        end_inference = time()\n        ref_latencies.append(end_inference - start_preprocess)\n    median_ref_latency = np.median(ref_latencies)\n\n    ### developed model's accuracy and latency\n    mfcc_processor = MFCC(**PREPROCESSING_ARGS)\n    interpreter = tf.lite.Interpreter(model_path=f'{ROOT_FOLDER}/tflite_models/{tflite_model_name}.tflite')\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    developed_model_latencies = []\n    for i in range(100):\n        start_preprocess = time()\n        mfccs = mfcc_processor.get_mfccs(audio)\n        mfccs = tf.expand_dims(mfccs, 0)\n        mfccs = tf.expand_dims(mfccs, -1)\n        interpreter.set_tensor(input_details[0]['index'], mfccs)\n        interpreter.invoke()\n        output = interpreter.get_tensor(output_details[0]['index'])\n        end_inference = time()\n        developed_model_latencies.append(end_inference - start_preprocess)\n    median_developed_model_latency = np.median(developed_model_latencies)\n\n    # measuring the savings\n    savings = 100 * (median_ref_latency - median_developed_model_latency) / median_ref_latency\n    return savings","block_group":"b8a2c707e4944edfb97e61e09e0ba1c8","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"191119e1","execution_start":1704123937430,"execution_millis":18,"deepnote_to_be_reexecuted":false,"cell_id":"f09cb2c382a6406d9586aa88ebd0ea8e","deepnote_cell_type":"code"},"source":"def develop_model(PREPROCESSING_ARGS, TRAINING_ARGS, OPTIMIZATION_ARGS):\n\n    ### preprocessing objects\n    audio_reader = AudioReader(tf.int16, PREPROCESSING_ARGS['sampling_rate'])\n    mfcc_processor = MFCC(**PREPROCESSING_ARGS)\n    def prepare_for_training(feature, label):\n        feature = tf.expand_dims(feature, -1)\n        label_id = tf.argmax(label == LABELS)\n        return feature, label_id\n\n    ### datasets\n    print('Loading the datasets...')\n    train_ds = tf.data.Dataset.list_files(f'{DATABASE_FOLDER}/yn-train/*')\n    test_ds = tf.data.Dataset.list_files(f'{DATABASE_FOLDER}/yn-test/*')\n    train_ds = (train_ds\n                .map(audio_reader.get_audio_and_label)\n                .map(mfcc_processor.get_mfccs_and_label)\n                .map(prepare_for_training)\n                .batch(TRAINING_ARGS['batch_size'])\n                .cache())\n    test_ds = (test_ds\n                .map(audio_reader.get_audio_and_label)\n                .map(mfcc_processor.get_mfccs_and_label)\n                .map(prepare_for_training)\n                .batch(TRAINING_ARGS['batch_size']))\n    \n    ### defining the model\n    print()\n    print('Defining the model...')\n    for example_batch, example_labels in train_ds.take(1):\n        shape = example_batch.shape\n    if OPTIMIZATION_ARGS['use_depthwise']:\n        model = tf.keras.Sequential([\n            tf.keras.layers.Input(shape=shape[1:]),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[1, 1], strides=[1, 1], use_bias=False),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(int(100 * OPTIMIZATION_ARGS['beta'])),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n    else:\n        model = tf.keras.Sequential([\n            tf.keras.layers.Input(shape=shape[1:]),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Conv2D(filters=int(128 * OPTIMIZATION_ARGS['alpha']), kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(int(100 * OPTIMIZATION_ARGS['beta'])),\n            tf.keras.layers.Dense(1, activation='sigmoid')\n        ])\n\n    ### quantization aware model\n    print()\n    print('Applying QAT...')\n    quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n\n    ### training the model\n    print()\n    print('Training the model...')\n    linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n        initial_learning_rate = TRAINING_ARGS['initial_learning_rate'],\n        end_learning_rate = TRAINING_ARGS['end_learning_rate'],\n        decay_steps = len(train_ds) * TRAINING_ARGS['epochs'],\n    )\n    quant_aware_model.compile(\n        loss = tf.losses.BinaryCrossentropy(from_logits=False),\n        optimizer = tf.optimizers.Adam(learning_rate=linear_decay),\n        metrics = [tf.metrics.BinaryAccuracy()]\n    )\n    history = quant_aware_model.fit(\n        train_ds,\n        epochs = TRAINING_ARGS['epochs']\n    )\n    pre_quant_training_accuracy = history.history['binary_accuracy'][-1] # accuracy of the last epoch\n\n    ### testing the model\n    print()\n    print('Testing the model...')\n    _, pre_quant_test_accuracy = quant_aware_model.evaluate(test_ds)\n\n    ### storing the model\n    print()\n    print('Storing the model...')\n    quant_aware_model.save(f'{ROOT_FOLDER}/saved_models/{MODEL_NAME}')\n\n    ### converting the model\n    print()\n    print('Converting the model...')\n    converter = tf.lite.TFLiteConverter.from_saved_model(f'{ROOT_FOLDER}/saved_models/{MODEL_NAME}')\n    if OPTIMIZATION_ARGS['use_post_training_optimization']:\n        # we need to give to the converter a representative dataset so that the\n        # activations quantization can be done (weights quantization dont need this).\n        representative_train_ds = tf.data.Dataset.list_files(f'{DATABASE_FOLDER}/yn-train/*')\n        representative_train_ds = (representative_train_ds\n                                    .map(audio_reader.get_audio_and_label)\n                                    .map(mfcc_processor.get_mfccs_and_label)\n                                    .map(prepare_for_training)\n                                    .cache())\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        def representative_data_gen():\n            for input_value, label in representative_train_ds.take(1000):\n                input_value_batched = tf.expand_dims(input_value, axis=0)\n                yield [input_value_batched]\n        converter.representative_dataset = representative_data_gen\n    tflite_model = converter.convert()\n    with open(f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite', 'wb') as fp:\n        fp.write(tflite_model)\n\n    ### getting the final accuracy\n    print()\n    print('Getting the TFLite model accuracy...')\n    interpreter = tf.lite.Interpreter(model_path=f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite')\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    predictionOutcomes = []\n    for currentFile in os.listdir(f'{DATABASE_FOLDER}/yn-test'):\n        fileName, fileExtension = os.path.splitext(currentFile)\n        if fileExtension == '.wav':\n            path = os.path.join(f'{DATABASE_FOLDER}/yn-test', currentFile)\n            audio, label = audio_reader.get_audio_and_label(path)\n            mfccs = mfcc_processor.get_mfccs(audio)\n            mfccs = tf.expand_dims(mfccs, 0)\n            mfccs = tf.expand_dims(mfccs, -1)\n            interpreter.set_tensor(input_details[0]['index'], mfccs)\n            interpreter.invoke()\n            output = interpreter.get_tensor(output_details[0]['index'])\n            predicted_probability = output[0][0]\n            predicted_label = int(predicted_probability > 0.5)\n            binary_label = int(label == LABELS[1])\n            predictionOutcomes.append(binary_label == predicted_label)\n    accuracy = 100 * sum(predictionOutcomes) / len(predictionOutcomes)\n\n    ### comparing the developed model to the reference model\n    print()\n    print('Benchmarking the developed TFLite model...')\n    savings = compare_to_reference(MODEL_NAME, PREPROCESSING_ARGS)\n\n    ### compressing and storing the tflite model\n    print('Compressing TFLite model...')\n    with zipfile.ZipFile(f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite.zip', 'w', compression=zipfile.ZIP_DEFLATED) as fp:\n        fp.write(f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite')\n\n    ### getting the tflite model size\n    size_uncompressed = os.path.getsize(f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite') / 1024\n    size_compressed = os.path.getsize(f'{ROOT_FOLDER}/tflite_models/{MODEL_NAME}.tflite.zip') / 1024\n\n    ### returning the final results\n    return pre_quant_training_accuracy, pre_quant_test_accuracy, accuracy, savings, size_uncompressed, size_compressed","block_group":"57ebf31e25fc4cc5b8b9f4e255e93bb9","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1503ce81bde0447692e8e423a3e4d8cd","deepnote_cell_type":"text-cell-h1"},"source":"# Training and Generating the TFLite Model","block_group":"60f26ecc314b4193bfcaa87d148c041c"},{"cell_type":"code","metadata":{"source_hash":"4a48c923","execution_start":1704123937494,"execution_millis":67310,"deepnote_to_be_reexecuted":false,"cell_id":"d510bdc58aec4f1da7c3d1b3ac4960dd","deepnote_cell_type":"code"},"source":"### setting the arguments based on the results discussed in the report\nPREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.064,\n    'frame_step_in_s': 0.016,\n    'num_mel_bins': 10,\n    'num_coefficients': 13,\n    'lower_frequency': 0,\n    'upper_frequency': 4000\n}\nTRAINING_ARGS = {\n    'batch_size': 20,\n    'initial_learning_rate': 1e-2,\n    'end_learning_rate': 1e-5,\n    'epochs': 20\n}\nOPTIMIZATION_ARGS = {\n    'use_depthwise': True,\n    'use_post_training_optimization': True,\n    'alpha': 0.5,\n    'beta': 0.05\n}\n\n### developing the model\npre_quant_training_accuracy, pre_quant_test_accuracy, accuracy, savings, size_uncompressed, size_compressed = develop_model(PREPROCESSING_ARGS, TRAINING_ARGS, OPTIMIZATION_ARGS)\n\n### checking the validity\nvalidity = all([\n    accuracy > 98.9,\n    savings > 35,\n    size_compressed < 25,\n])\n\n### printing\nprint()\nprint('Final Result...')\nprint(f'  Model Name: {MODEL_NAME}')\nprint(f'  Training Accuracy (Before QAT): {pre_quant_training_accuracy:.4f}')\nprint(f'  Test Accuracy (Before QAT): {pre_quant_test_accuracy:.4f}')\nprint(f'  Accuracy: {accuracy:.4f}%')\nprint(f'  Savings: {savings:.4f}%')\nprint(f'  Size (Uncompressed): {size_uncompressed:.4f}KB')\nprint(f'  Size (Compressed): {size_compressed:.4f}KB')\nprint(f'  Validity: {validity}')","block_group":"0c9c8b16ef974a2fbeb66f9ace967b28","execution_count":6,"outputs":[{"name":"stdout","text":"Loading the datasets...\n2024-01-01 15:45:37.936420: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX AVX2 FMA\n2024-01-01 15:45:37.938108: W tensorflow_io/core/kernels/audio_video_mp3_kernels.cc:271] libmp3lame.so.0 or lame functions are not available\n\nDefining the model...\n2024-01-01 15:45:39.164242: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\nApplying QAT...\n\nTraining the model...\nEpoch 1/20\n80/80 [==============================] - 7s 71ms/step - loss: 0.3623 - binary_accuracy: 0.8500\nEpoch 2/20\n80/80 [==============================] - 2s 25ms/step - loss: 0.2449 - binary_accuracy: 0.9112\nEpoch 3/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.1607 - binary_accuracy: 0.9369\nEpoch 4/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.1030 - binary_accuracy: 0.9625\nEpoch 5/20\n80/80 [==============================] - 2s 25ms/step - loss: 0.1011 - binary_accuracy: 0.9656\nEpoch 6/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0758 - binary_accuracy: 0.9681\nEpoch 7/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0531 - binary_accuracy: 0.9812\nEpoch 8/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0580 - binary_accuracy: 0.9781\nEpoch 9/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0519 - binary_accuracy: 0.9812\nEpoch 10/20\n80/80 [==============================] - 2s 25ms/step - loss: 0.0357 - binary_accuracy: 0.9894\nEpoch 11/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0286 - binary_accuracy: 0.9900\nEpoch 12/20\n80/80 [==============================] - 2s 25ms/step - loss: 0.0312 - binary_accuracy: 0.9900\nEpoch 13/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0255 - binary_accuracy: 0.9919\nEpoch 14/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0193 - binary_accuracy: 0.9925\nEpoch 15/20\n80/80 [==============================] - 2s 25ms/step - loss: 0.0176 - binary_accuracy: 0.9956\nEpoch 16/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0096 - binary_accuracy: 0.9975\nEpoch 17/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0079 - binary_accuracy: 0.9975\nEpoch 18/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0065 - binary_accuracy: 0.9981\nEpoch 19/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0058 - binary_accuracy: 0.9981\nEpoch 20/20\n80/80 [==============================] - 2s 24ms/step - loss: 0.0057 - binary_accuracy: 0.9981\n\nTesting the model...\n10/10 [==============================] - 1s 55ms/step - loss: 0.0235 - binary_accuracy: 0.9900\n\nStoring the model...\nINFO:tensorflow:Assets written to: ./saved_models/model1/assets\nINFO:tensorflow:Assets written to: ./saved_models/model1/assets\n\nConverting the model...\n2024-01-01 15:46:35.336138: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n2024-01-01 15:46:35.336179: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n2024-01-01 15:46:35.486500: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/model1\n2024-01-01 15:46:35.624377: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n2024-01-01 15:46:35.624403: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./saved_models/model1\n2024-01-01 15:46:35.633021: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n2024-01-01 15:46:35.635931: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n2024-01-01 15:46:36.066223: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ./saved_models/model1\n2024-01-01 15:46:36.096921: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 610434 microseconds.\n2024-01-01 15:46:36.121590: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n2024-01-01 15:46:39.061561: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\nfully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n\nGetting the TFLite model accuracy...\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n\nBenchmarking the developed TFLite model...\nCompressing TFLite model...\n\nFinal Result...\n  Model Name: model1\n  Training Accuracy (Before QAT): 0.9981\n  Test Accuracy (Before QAT): 0.9900\n  Accuracy: 99.0000%\n  Savings: 39.7917%\n  Size (Uncompressed): 24.7266KB\n  Size (Compressed): 14.2598KB\n  Validity: True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=abe8bec9-2977-4d96-847e-1817af901527' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"8b73cf9e371e446b89cedea2b316bea9","deepnote_execution_queue":[]}}